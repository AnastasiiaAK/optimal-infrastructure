{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 14 13:56:11 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:5E:00.0 Off |                    0 |\r\n",
      "| N/A   67C    P0    29W /  70W |  15101MiB / 15109MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting launch_ddp_MNIST.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp_MNIST.py\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--nodes\", default = 1, type = int, metavar = \"N\", help = \"кол-во обработчиков (default: 1)\")\n",
    "    parser.add_argument(\"-g\", \"--gpus\", default = 1, type = int, help = \"число гпу на каждом обработчике\")\n",
    "    parser.add_argument(\"-nr\", \"--nr\", default = 0, type = int, help = \"глобальный ранг\") # для каждого новог процесса создается новаывй (но это просиходит под капотом)\n",
    "    parser.add_argument(\"--epochs\", default = 2, type = int, help = \"колво эпох обучения\", metavar = \"N\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    args.world_size = args.gpus * args.nodes\n",
    "    os.environ[\"MASTER_ADDR\"] = '0.0.0.0' # обявление мастер ноды\n",
    "    os.environ[\"MASTER_PORT\"] = '8809'\n",
    "    # create processes\n",
    "    mp.spawn(train, nprocs = args.gpus, args = (args,)) # paramneter server\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # convoluution\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,kernel_size = 5, stride = 1, padding = 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2))\n",
    "        \n",
    "        #convolution\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16,32,kernel_size = 5, stride = 1, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2))\n",
    "        # full layer \n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def train(gpu, args):\n",
    "    rank = args.nr * args.gpus + gpu\n",
    "    dist.init_process_group(backend = \"nccl\", init_method = \"env://\", world_size = args.world_size, rank = rank)\n",
    "    torch.manual_seed(0)\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.cuda(gpu)\n",
    "    batch_size = 100\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform = transforms.ToTensor(),download=True)\n",
    "    \n",
    "    # для того чтобы постоянно нге считыыввать весь датасет сущесвтует DistributedSampler который считывыает только те блоки которые им нужны\n",
    "    \n",
    "    # привродим текущий датасет, колв-во устройств и каким рангов производиться текущая обработка\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=args.world_size, rank=rank)\n",
    "    \n",
    "    # именно он юудет производить считку данных\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, num_workers=0, pin_memory=True, sampler=train_sampler)\n",
    "    \n",
    "    \n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader) # кол-во шагов в нашем загрузчике данных\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(epoch, i, loss, gpu)\n",
    "\n",
    "    if gpu == 0:\n",
    "        print(\"Обучениен завершено за \" + str(datetime.now() - start))\n",
    "            \n",
    "            \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "0 99 tensor(2.1141, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "0 199 tensor(2.0650, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "0 299 tensor(1.9299, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "0 399 tensor(1.7870, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "0 499 tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "0 599 tensor(1.5558, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 99 tensor(1.5122, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 199 tensor(1.3986, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 299 tensor(1.3406, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 399 tensor(1.2922, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 499 tensor(1.2061, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "1 599 tensor(1.1375, device='cuda:0', grad_fn=<NllLossBackward>) 0\n",
      "Обюучениен завершено за 0:00:13.155057\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp_MNIST.py -n 1 -g 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
