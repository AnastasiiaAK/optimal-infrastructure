{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f8aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting horovod.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile horovod.py\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    \n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "\n",
    "# парамеры специфичные для оптиизатора\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "def train_one_epoch(model, device, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch\", epoch)\n",
    "            \n",
    "            \n",
    "from time import time\n",
    "import os\n",
    "\n",
    "# function os saving of checkpoints\n",
    "LOG_DIR = os.path.join(\"./logs/\", str(time()), \"MNISTDemo\")\n",
    "\n",
    "os.makedirs(LOG_DIR)\n",
    "\n",
    "import horovod as hvd\n",
    "\n",
    "# function for savinf this checkpoints in this path\n",
    "def save_checkpoint(model, optimizer, epoch):\n",
    "    filepath = LOG_DIR + '/checkpoint - {epoch}.pth.tar'.format(epoch=epoch)\n",
    "    state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    \n",
    "    torch.save(state, filepath)\n",
    "    \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(learning_rate):\n",
    "    device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_dataset = torchvision.datasets.MNIST('data', train=True,download = True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, num_workers=0, pin_memory=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr= learning_rate,momentum = momentum)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(model, device, data_loader, optimizer, epoch)\n",
    "        save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "        \n",
    "\n",
    "def train_hvd(learning_rate):\n",
    "    hvd.init()\n",
    "    \n",
    "    device = torch.device((\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "        \n",
    "        \n",
    "    train_dataset = datasets.MNIST(\n",
    "    root = 'data-%d'% hvd.rank(),\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler = train_sampler)\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum = momentum)\n",
    "    \n",
    "    # это позволяет синхронизирвоат обработсик в мормент посмле гардлиетов\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters = model.named_parameters())\n",
    "    \n",
    "    \n",
    "    # ставим для моджел пармеитры одинаковые\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank = 0)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "        if hvd.rank() == 0:\n",
    "            # так как все синхронизировано то сохланеямтолктона одлном процессроре\n",
    "            save_checkpoint(model, optimizer, epoch)\n",
    "            \n",
    "            \n",
    "def train_hvd(learning_rate):\n",
    "  \n",
    "  # Initialize Horovod\n",
    "    #hvd.init()  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "    if device.type == 'cuda':\n",
    "    # Pin GPU to local rank\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "    # Use different root directory for each worker to avoid conflicts\n",
    "    root='data-%d'% hvd.rank(),  \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    )\n",
    "\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "    # Configure the sampler so that each worker gets a distinct sample of the input dataset\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    # Use train_sampler to load a different sample of data on each worker\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    # The effective batch size in synchronous distributed training is scaled by the number of workers\n",
    "    # Increase learning_rate to compensate for the increased batch size\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
    "\n",
    "    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "\n",
    "    # Broadcast initial parameters so all workers start with the same parameters\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "    # Save checkpoints only on worker 0 to prevent conflicts between workers\n",
    "    if hvd.rank() == 0:\n",
    "        save_checkpoint(hvd_log_dir, model, optimizer, epoch)\n",
    "            \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train_hvd(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62614bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from horovod import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ce0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5288feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: horovodrun [-h] [-v] -np NP [-cb] [--disable-cache]\r\n",
      "                  [--start-timeout START_TIMEOUT] [--network-interface NICS]\r\n",
      "                  [--output-filename OUTPUT_FILENAME] [--verbose]\r\n",
      "                  [--config-file CONFIG_FILE] [-p SSH_PORT]\r\n",
      "                  [-i SSH_IDENTITY_FILE]\r\n",
      "                  [--fusion-threshold-mb FUSION_THRESHOLD_MB]\r\n",
      "                  [--cycle-time-ms CYCLE_TIME_MS]\r\n",
      "                  [--cache-capacity CACHE_CAPACITY]\r\n",
      "                  [--hierarchical-allreduce | --no-hierarchical-allreduce]\r\n",
      "                  [--hierarchical-allgather | --no-hierarchical-allgather]\r\n",
      "                  [--autotune] [--autotune-log-file AUTOTUNE_LOG_FILE]\r\n",
      "                  [--autotune-warmup-samples AUTOTUNE_WARMUP_SAMPLES]\r\n",
      "                  [--autotune-steps-per-sample AUTOTUNE_STEPS_PER_SAMPLE]\r\n",
      "                  [--autotune-bayes-opt-max-samples AUTOTUNE_BAYES_OPT_MAX_SAMPLES]\r\n",
      "                  [--autotune-gaussian-process-noise AUTOTUNE_GAUSSIAN_PROCESS_NOISE]\r\n",
      "                  [--min-np MIN_NP] [--max-np MAX_NP] [--slots-per-host SLOTS]\r\n",
      "                  [--elastic-timeout ELASTIC_TIMEOUT]\r\n",
      "                  [--reset-limit RESET_LIMIT]\r\n",
      "                  [--timeline-filename TIMELINE_FILENAME]\r\n",
      "                  [--timeline-mark-cycles] [--no-stall-check]\r\n",
      "                  [--stall-check-warning-time-seconds STALL_CHECK_WARNING_TIME_SECONDS]\r\n",
      "                  [--stall-check-shutdown-time-seconds STALL_CHECK_SHUTDOWN_TIME_SECONDS]\r\n",
      "                  [--mpi-threads-disable] [--mpi-args MPI_ARGS] [--tcp]\r\n",
      "                  [--binding-args BINDING_ARGS]\r\n",
      "                  [--num-nccl-streams NUM_NCCL_STREAMS]\r\n",
      "                  [--thread-affinity THREAD_AFFINITY]\r\n",
      "                  [--gloo-timeout-seconds GLOO_TIMEOUT_SECONDS]\r\n",
      "                  [--log-level {TRACE,DEBUG,INFO,WARNING,ERROR,FATAL}]\r\n",
      "                  [--log-without-timestamp | -prefix-timestamp]\r\n",
      "                  [-H HOSTS | -hostfile HOSTFILE | --host-discovery-script HOST_DISCOVERY_SCRIPT]\r\n",
      "                  [--gloo | --mpi | --jsrun]\r\n",
      "                  ...\r\n",
      "horovodrun: error: the following arguments are required: -np/--num-proc\r\n"
     ]
    }
   ],
   "source": [
    "!horovodrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f439731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hvd(learning_rate):\n",
    "  \n",
    "  # Initialize Horovod\n",
    "    hvd.init()  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "    if device.type == 'cuda':\n",
    "    # Pin GPU to local rank\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "    # Use different root directory for each worker to avoid conflicts\n",
    "    root='data-%d'% hvd.rank(),  \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    )\n",
    "\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "    # Configure the sampler so that each worker gets a distinct sample of the input dataset\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    # Use train_sampler to load a different sample of data on each worker\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    # The effective batch size in synchronous distributed training is scaled by the number of workers\n",
    "    # Increase learning_rate to compensate for the increased batch size\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
    "\n",
    "    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "\n",
    "    # Broadcast initial parameters so all workers start with the same parameters\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "    # Save checkpoints only on worker 0 to prevent conflicts between workers\n",
    "    if hvd.rank() == 0:\n",
    "        save_checkpoint(hvd_log_dir, model, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036e873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: horovodrun [-h] [-v] -np NP [-cb] [--disable-cache]\r\n",
      "                  [--start-timeout START_TIMEOUT] [--network-interface NICS]\r\n",
      "                  [--output-filename OUTPUT_FILENAME] [--verbose]\r\n",
      "                  [--config-file CONFIG_FILE] [-p SSH_PORT]\r\n",
      "                  [-i SSH_IDENTITY_FILE]\r\n",
      "                  [--fusion-threshold-mb FUSION_THRESHOLD_MB]\r\n",
      "                  [--cycle-time-ms CYCLE_TIME_MS]\r\n",
      "                  [--cache-capacity CACHE_CAPACITY]\r\n",
      "                  [--hierarchical-allreduce | --no-hierarchical-allreduce]\r\n",
      "                  [--hierarchical-allgather | --no-hierarchical-allgather]\r\n",
      "                  [--autotune] [--autotune-log-file AUTOTUNE_LOG_FILE]\r\n",
      "                  [--autotune-warmup-samples AUTOTUNE_WARMUP_SAMPLES]\r\n",
      "                  [--autotune-steps-per-sample AUTOTUNE_STEPS_PER_SAMPLE]\r\n",
      "                  [--autotune-bayes-opt-max-samples AUTOTUNE_BAYES_OPT_MAX_SAMPLES]\r\n",
      "                  [--autotune-gaussian-process-noise AUTOTUNE_GAUSSIAN_PROCESS_NOISE]\r\n",
      "                  [--min-np MIN_NP] [--max-np MAX_NP] [--slots-per-host SLOTS]\r\n",
      "                  [--elastic-timeout ELASTIC_TIMEOUT]\r\n",
      "                  [--reset-limit RESET_LIMIT]\r\n",
      "                  [--timeline-filename TIMELINE_FILENAME]\r\n",
      "                  [--timeline-mark-cycles] [--no-stall-check]\r\n",
      "                  [--stall-check-warning-time-seconds STALL_CHECK_WARNING_TIME_SECONDS]\r\n",
      "                  [--stall-check-shutdown-time-seconds STALL_CHECK_SHUTDOWN_TIME_SECONDS]\r\n",
      "                  [--mpi-threads-disable] [--mpi-args MPI_ARGS] [--tcp]\r\n",
      "                  [--binding-args BINDING_ARGS]\r\n",
      "                  [--num-nccl-streams NUM_NCCL_STREAMS]\r\n",
      "                  [--thread-affinity THREAD_AFFINITY]\r\n",
      "                  [--gloo-timeout-seconds GLOO_TIMEOUT_SECONDS]\r\n",
      "                  [--log-level {TRACE,DEBUG,INFO,WARNING,ERROR,FATAL}]\r\n",
      "                  [--log-without-timestamp | -prefix-timestamp]\r\n",
      "                  [-H HOSTS | -hostfile HOSTFILE | --host-discovery-script HOST_DISCOVERY_SCRIPT]\r\n",
      "                  [--gloo | --mpi | --jsrun]\r\n",
      "                  ...\r\n",
      "horovodrun: error: the following arguments are required: -np/--num-proc\r\n"
     ]
    }
   ],
   "source": [
    "!horovodrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a0877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b591e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 13:28:54.809817: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-14 13:28:54.809844: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-10-14 13:28:56.059723: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-14 13:28:56.059754: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[0]<stderr>:Traceback (most recent call last):\n",
      "[0]<stderr>:  File \"horovod.py\", line 176, in <module>\n",
      "[0]<stderr>:    train_hvd(0.001)\n",
      "[0]<stderr>:  File \"horovod.py\", line 143, in train_hvd\n",
      "[0]<stderr>:    root='data-%d'% hvd.rank(),  \n",
      "[0]<stderr>:AttributeError: module 'horovod' has no attribute 'rank'\n",
      "Process 0 exit with status code 1.\n",
      "Terminating remaining workers after failure of Process 0.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anastasiia/anaconda3/bin/horovodrun\", line 8, in <module>\n",
      "    sys.exit(run_commandline())\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 770, in run_commandline\n",
      "    _run(args)\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 760, in _run\n",
      "    return _run_static(args)\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 617, in _run_static\n",
      "    _launch_job(args, settings, nics, command)\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 730, in _launch_job\n",
      "    run_controller(args.use_gloo, gloo_run_fn,\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 706, in run_controller\n",
      "    gloo_run()\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/launch.py\", line 722, in gloo_run_fn\n",
      "    gloo_run(settings, nics, env, driver_ip, command)\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/gloo_run.py\", line 298, in gloo_run\n",
      "    launch_gloo(command, exec_command, settings, nics, env, server_ip)\n",
      "  File \"/home/anastasiia/anaconda3/lib/python3.8/site-packages/horovod/runner/gloo_run.py\", line 282, in launch_gloo\n",
      "    raise RuntimeError('Horovod detected that one or more processes exited with non-zero '\n",
      "RuntimeError: Horovod detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:\n",
      "Process name: 0\n",
      "Exit code: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np 1 python3 horovod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04014878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ad3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
